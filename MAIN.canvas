{
	"nodes":[
		{"id":"6966cc00003cb877","type":"file","file":"pages/DATA WAREHOUSE ARCHITECTURES.md","x":560,"y":100,"width":400,"height":400},
		{"id":"a6fe155017f99580","type":"text","text":"# DATAMINING","x":-106,"y":-80,"width":246,"height":41},
		{"id":"e157b776062379a6","type":"file","file":"pages/CONCEPTUAL MODELING.md","x":580,"y":600,"width":400,"height":400},
		{"id":"5f2ebe869cec563b","type":"file","file":"pages/CRISP DM METHODOLOGY.md","x":440,"y":1100,"width":400,"height":400},
		{"id":"e847392230486bab","type":"file","file":"pages/BUSINESS INTELLIGENCE AND DATA WAREHOUSE.md","x":-140,"y":100,"width":400,"height":400},
		{"id":"7065b245fa4f0b89","type":"file","file":"pages/DATA LAKES.md","x":-60,"y":-680,"width":400,"height":400},
		{"id":"ca9a8fd4934c0510","type":"file","file":"pages/OLAP.md","x":-640,"y":-680,"width":400,"height":400},
		{"id":"6ecf75efb9a7303c","type":"text","text":"# MACHINE LEARNING","x":1140,"y":-1040,"width":209,"height":84},
		{"id":"b74175251077cc37","type":"file","file":"pages/DATA TYPES.md","x":580,"y":-680,"width":400,"height":400},
		{"id":"d8f89172f4b02b7a","type":"text","text":"# SUPERVISED LEARNING","x":1555,"y":-722,"width":250,"height":84},
		{"id":"23e64f164cb59adc","type":"file","file":"pages/CLASSIFICATION.md","x":1155,"y":-480,"width":400,"height":400},
		{"id":"ecae646a14a92104","type":"file","file":"pages/DECISION TREES.md","x":1155,"y":120,"width":400,"height":400},
		{"id":"392d016a92152695","type":"text","text":"\n# Naive Bayes Classifier \n\nBasato (su metodi statistici) particolarmente sul teorema di Bayes.\n![](Pasted%20image%2020231230105024.png)\n\ncon il training valuti Pr(H) creando il modello di classificazione mentre a runtime vedi Pr(E|H)\n\nuna prima versione considera tutti gli attributi assumendo siano indipendenti gli uni dagli altri ->poco applicabile ma di facile comprensione.\nUn altro problema è che se un valore v di un attributo d non abbare negli elementi della classe C allora P(d=v|c)=0 la probabilità della classe conoscendo l'attrabuto crolla a zero\n\nmodi per evitare il problema: \n1. Laplace smoothing \n![](Pasted%20image%2020231230105901.png)\ncon alfa=0 otteniamo la formula standard non smussata, più è alto alfa maggiore importanza si da ai valori nel dataset .\n\nNel caso di valori mancanti non è necessario cancellare le istanze:\n\t-Se è un valore del Training set ometto il valore dell'attributo (ottengo una Pr maggiore che si compenserà con la normalizzazione).\n\t-Se è un valore del Test set lo trascuro.\n\n2. Nel caso di valori numerici non si può usare il metodo basato sulle frequenze.\n\tAssumiamo che i valori si trovino su una distribuzione Gaussiana, descriviamo la distribuzione del valore degli attributi sulla base del valor medio e della varianza degli attributi all'interno di ogni classe.\n\tLa distribuzione di valore per un atributo di una classe è :\n\n\t![](Pasted%20image%2020231230111308.png)\n\nQuindi si imposta una soglia per un valore di interesse (ad esempio impostare di poter giocare a tennis se la Temperatura è superiore ai 20 gradi) e si calcola quella che viene detta Densità di Probabilità:\n![](Pasted%20image%2020231230111910.png)\n\nChe rappresenta la Pr(E|H) del teorema di Bayes.\nProbabilità e Densità si differenziano :\n\t- Su domini continui la probabilità che una variabile assuma esattamente un valore è 0\n\t- La densità si può assumere come la probabilità che la variabile assuma un valore nell'intorno della soglia (diverso da 0)\n\t- Se un valore è mancante la deviazione standard è basata solo sui valori presenti","x":1680,"y":0,"width":600,"height":400},
		{"id":"eb24e2d0e03edc07","type":"text","text":"# Valutare un classificatore probabilistico\n\nGenera una probabilità per ogni possibile classe (simile al classificatore Crisp).\nPossiamo settare un Threashold di probabilità oltre il quale si restituisce il valore Binario.\n\n- Lift Chart: individui un campione di N individui ed effettui il test e ripeti usando un classificatore casuale. Disegnando il grafico maggiore è l'area tra le due curve migliore è la classificazione.\n\t![](Pasted%20image%2020231230150751.png)\n- ROC Curve : simula il tradeoff tra Veri Positivi e Falsi Positivi in una trasmissione con canale affetto da rumore. \n\t![](Pasted%20image%2020231230150901.png)Calcola e traccia il grafico ponendo nell'asse X la Pr dei falsi positivi (specificity) e nell'asse delle Y la Pr dei veri positivi (sensitività) al variare della soglia di threshold, maggiore è la distanza tra le due curve migliore è il classificatore->migliore il threshold usato.\n\t![](Pasted%20image%2020231230151003.png)![](Pasted%20image%2020231230151021.png)\n\t","x":1680,"y":480,"width":540,"height":600},
		{"id":"1893b93996ec7f0c","type":"text","text":"# Linear Perceptron\n\nCombinazione lineare di input pesati->rete combinatoria che in maniera ricorsiva modifica i pesi dati a ogni parametro dell'iperpiano fino a convergere al quello ottimo o ammissibile![](Pasted%20image%2020231230121407.png)\n\nQuindi dato un dataset con attributi numerici X e Y per classificarli in 2 classi li tracciamo come punti nel piano e cerchiamo di tracciare l'iperpiano che li divide\n![](Pasted%20image%2020231230121721.png)\n\nd1,d2....d*d* sono gli attributi w0,w1...wd sono i pesi da calibrare, il primo elemento ha sempre valore input=1, detto Bias serve ad ammettere iperpiano che non passano dall'origine. \nL'iperpiano è descritto da un set di pesi in una equazione lineare sugli attributi x0...xd ![](Pasted%20image%2020231230122322.png)\n\nl'algoritmo di apprendimento:\n![](Pasted%20image%2020231230122400.png)\n\nOgni modifica dei pesi sposta il piano verso individui mal classificati, se l'algoritmo è classificato male e dovrebbe essere positivo aggiungo ai pesi del percettrone il valore dei suoi attributi se è negativo sottraggo.\nL'algoritmo termina solo de il dataset  è linearmente Separabile quindi è bene impostare un limite temporale ammettendo errori.","x":2085,"y":-820,"width":560,"height":530},
		{"id":"ca084eb0880603f9","type":"text","text":"# Support Vector Machines (SVM)\n\nStudiate per superare il problema dei dataset linearmente separabili-> cerca un'ipersuperficie che divida gli individui del dataset ma la superficie è più complessa di un iperpiano.\n- prima idea: far cadere il requisito di linearità della supericie, ma si ottiene una classificazione molto dipendente dai dati del dataset con molto overfitting. Ma si può ottenere una soluzione valida mappando i punti da un dominio non llinearmente separabile a uno linearmente separabile con un numero di dimensioni maggiori del dominio originale: ![](Pasted%20image%2020231230124031.png) ![](Pasted%20image%2020231230130632.png)\n- altrimenti: si mantiene il requisito di linearità e si adotta il Maximum Margin Hyperplane si pensa di non adottare un singolo piano, ma piani multipli dove ogni singolo piano cerca di massimizzare il margine tra i gruppi di individui. Il margine (distanza tra piano e gruppo di individui) si valua considerando il convex hull (l’inviluppo convesso più grande che riesce a contenere tutti i punti del sottoinsieme). \n\t![](Pasted%20image%2020231230124428.png)\n\n- Oppure ancora si possono usare le Reti Neurali: composte da numerosi percettori collocati in gerarchia. ","x":2880,"y":-850,"width":560,"height":560},
		{"id":"70adf06c2620443c","x":3600,"y":-760,"width":620,"height":520,"type":"text","text":"# Reti Neurali\n\nPossiamo identificare un neurone come una rete combinatoria a soglia, restituisce 1 o 0 se supera o meno la soglia.  \n\t I valori in input, inseriti tramite le sinapsi, sono numeri reali pesati; \n\t Allo stesso modo sono pesati i dendriti, cioè le connessioni tra neuroni interne alla rete; \n\t La soglia è data da una funzione matematica, solitamente un Sigmoide (squashing function)![](Pasted%20image%2020231230124921.png) è continua, differenziabile e non lineare(così da creare una rete robusta in presenza di rumore).\nLa forma della funzione influenza il processo di apprendimento che consiste nel calibrare i pesi di sinapsi e dendriti basandosi sui dati del training set.\n![](Pasted%20image%2020231230145255.png)\nNella Feed-Forward multi-layered Network troviamo di base 3 layer:\n- Input Layer (nutrito dagli Input tramite le sinapsi)\n- Hidden Layer (nutrito dall'input layer tramite dentriti)\n- Output Layer (nutrito dall'hidden layer)\n\n![](Pasted%20image%2020231230145402.png)\ng= sigmoidi (funzioni di trasferimento)\nGli archi sono orientati (rete feed-forward) \nl'algoritmo di training:\n![](Pasted%20image%2020231230145700.png)\n\nnella rete vengono immessi tutti gli elementi del dataset e durante il processo i pesi vengono calibrati sulla base delle classi dai supervisori, ma la convergenza non è garantita.\n\n##### Calcolo dell'errore\nL'errore si calcola:\n![](Pasted%20image%2020231230150120.png)\nla modifica in retroazione fatta vuole minimizzare l'errore E(w) seguendo il gradiente della funzione che si calcola dalle derivate parziali del sigmoide: ![](Pasted%20image%2020231230150356.png) \n\n![](Pasted%20image%2020231230150251.png)\n\nSottrai la derivata per il parametro costante detto Learning Rate \n![](Pasted%20image%2020231230150551.png)\n\n"},
		{"id":"42219c5107e0a671","x":2179,"y":-1201,"width":401,"height":361,"type":"text","text":"# Tipi di Learning\n\n- Stocastico: una propagazione forward è semre seguita da un aggiornamento dei pesi (ciò introduce rumore nella minimizzazione del gradiente). Valido per learning online.\n- Batch : fatte alcune propagazioni forward si aggiornano i pesi, accumula il possibile errore sui pesi (di solito porta in maniera più rapida a una condizione di discesa costante del gradiente perché l’aggiornamento si basa sull’errore medio)"},
		{"id":"faf61d4c68087908","x":1805,"y":-1336,"width":335,"height":256,"type":"text","text":"# Scelte di progetto\n Numero di epoche: con epoca si intende l’elaborazione di tutto il training set, solitamente sono necessarie svariate epoche per arrivare a un buon classificatore; \n Numero di nodi nell’hidden layer; \n Struttura della rete; \n Costante di Learning Rate, a volte viene modificato tra un’epoca e l’altra per migliorare la precisione; \n Criterio di Stop: o Tutti i pesi sono “piccoli”; o Una soglia minima di errore di classificazione; o Time-out. "}
	],
	"edges":[
		{"id":"dae3895ac9e72af7","fromNode":"e847392230486bab","fromSide":"right","toNode":"6966cc00003cb877","toSide":"left"},
		{"id":"78f78305e211fc12","fromNode":"6966cc00003cb877","fromSide":"bottom","toNode":"e157b776062379a6","toSide":"top"},
		{"id":"7c13e51ede606698","fromNode":"6ecf75efb9a7303c","fromSide":"bottom","toNode":"b74175251077cc37","toSide":"top"},
		{"id":"4759f8bf2cccbb1b","fromNode":"a6fe155017f99580","fromSide":"bottom","toNode":"e847392230486bab","toSide":"top"},
		{"id":"b53221495ca2962f","fromNode":"e157b776062379a6","fromSide":"bottom","toNode":"5f2ebe869cec563b","toSide":"top"},
		{"id":"337949a05e535b3c","fromNode":"a6fe155017f99580","fromSide":"top","toNode":"ca9a8fd4934c0510","toSide":"bottom"},
		{"id":"c70c999878069bf6","fromNode":"a6fe155017f99580","fromSide":"top","toNode":"7065b245fa4f0b89","toSide":"bottom"},
		{"id":"a3e58451df4a6fdf","fromNode":"6ecf75efb9a7303c","fromSide":"bottom","toNode":"d8f89172f4b02b7a","toSide":"top"},
		{"id":"274a15065935847d","fromNode":"23e64f164cb59adc","fromSide":"bottom","toNode":"ecae646a14a92104","toSide":"top"},
		{"id":"4848b3c417280608","fromNode":"d8f89172f4b02b7a","fromSide":"bottom","toNode":"23e64f164cb59adc","toSide":"top"},
		{"id":"c3f54a4fbc7dae2d","fromNode":"23e64f164cb59adc","fromSide":"right","toNode":"392d016a92152695","toSide":"top"},
		{"id":"51d78ce42586a622","fromNode":"392d016a92152695","fromSide":"bottom","toNode":"eb24e2d0e03edc07","toSide":"top"},
		{"id":"3334d6a0915394ee","fromNode":"23e64f164cb59adc","fromSide":"right","toNode":"1893b93996ec7f0c","toSide":"left"},
		{"id":"5d1ce538f1695ba2","fromNode":"1893b93996ec7f0c","fromSide":"right","toNode":"ca084eb0880603f9","toSide":"left"},
		{"id":"61f23754f96d8af6","fromNode":"ca084eb0880603f9","fromSide":"right","toNode":"70adf06c2620443c","toSide":"left"},
		{"id":"93aa835512150d7b","fromNode":"d8f89172f4b02b7a","fromSide":"right","toNode":"42219c5107e0a671","toSide":"left"},
		{"id":"5eda2a39f110a092","fromNode":"d8f89172f4b02b7a","fromSide":"right","toNode":"faf61d4c68087908","toSide":"left"}
	]
}